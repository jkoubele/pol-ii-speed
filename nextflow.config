params {
    outdir = "./results"
    reports_subdir = 'pipeline_info'

    container_tools = "jkoubele/pol-ii-speed-tools:0.2.0"
    container_python = "jkoubele/pol-ii-speed-python:0.1.0"
    container_r = "jkoubele/pol-ii-speed-r:0.2.0"

    workDir = "./work"
    // Optional full-path override for lineage storage, default (if null) is in .lineage next to params.workDir
    lineage_dir = null

    fit_model_cpus = 6 // We override for our cluster
    fit_model_memory  = '10 GB' // We override for our cluster

}

workDir = params.workDir
reportsDir = "${params.outdir}/${params.reports_subdir}"

docker {
    enabled = true
    runOptions = "-u \$(id -u):\$(id -g)"
}


report {
    enabled = true
    file = "${reportsDir}/report.html"
    overwrite = true
}

timeline {
    enabled = true
    file = "${reportsDir}/timeline.html"
    overwrite = true
}

trace {
    enabled = true
    file = "${reportsDir}/trace.txt"
    overwrite = true
}

dag {
    enabled = true
    file = "${reportsDir}/dag.png"
    overwrite = true
}

lineage {
    enabled = true
    store.location = {
        // If user set a custom full path, use that
        if (params.lineage_dir) {
            return params.lineage_dir
        }
        // Otherwise default to <dataset_root>/.lineage, where <dataset_root> is the parent of workDir
        if (!params.workDir) {
            // optional: fallback if workDir is not set
            return '.lineage'
        }
        def dataset_root = new File(params.workDir).getParent()
        return "${dataset_root}/.lineage"
    }()
}


process {

    withName: FastQC {
        container = "quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0"
        cpus = 2
        memory = '4 GB'
    }

    withName: MultiQC {
        container = "multiqc/multiqc:v1.30"
        cpus = 4
        memory = '8 GB'
    }

    withName: ExtractIntronsFromGTF {
        container = params.container_python
        cpus = 1
        memory = '6 GB'
    }

    withName: GetGeneIDsFromGTF {
        container = params.container_r
        cpus = 1
        memory = '6 GB'
    }

    withName: BuildStarIndex {
        container = params.container_tools
        cpus = 16
        memory = '200 GB'
    }

    withName: BuildSalmonIndex {
        container = params.container_tools
        cpus = 16
        memory = '200 GB'
    }

    withName: PrepareTx2Gene {
        container = params.container_r
        cpus = 2
        memory = '8 GB'
    }

    withName: CreateGenomeFastaIndex {
        container = params.container_tools
        cpus = 1
        memory = '4 GB'
    }

    withName: STARAlign {
        container = params.container_tools
        cpus = 12
        memory = '120 GB'
    }

    withName: ExtractIntronicReads {
        container = params.container_python
        cpus = 4
        memory = '10 GB'
    }

    withName: RemoveIntronicReadsFromFASTQ{
        container = params.container_python
        cpus = 4
        memory = '12 GB'
    }

    withName: SalmonQuantification {
        container = params.container_tools
        cpus = 8
        memory = '20 GB'
    }

    withName: ComputeCoverage {
        container = params.container_tools
        cpus = 2
        memory = '8 GB'
    }

    withName: RescaleCoverage{
        container = params.container_r
        cpus = 2
        memory = '16 GB'
    }

    withName: AggregateReadCounts{
        container = params.container_r
        cpus = 2
        memory = '16 GB'
    }

    withName: CreateDesignMatrices{
        container = params.container_r
        cpus = 1
        memory = '6 GB'
    }

    withName: SplitGeneNames{
        container = params.container_r
        cpus = 1
        memory = '6 GB'
    }

    withName: FitModel {
        container = params.container_python
        cpus = { params.fit_model_cpus }
        memory = { params.fit_model_memory }
    }

    withName: FitRegularizedModel {
        container = params.container_python
        cpus = { params.fit_model_cpus }
        memory = { params.fit_model_memory }
    }

    withName: MergeModelResultChunks{
        container = params.container_r
        cpus = 1
        memory = '8 GB'
    }

    withName: AdaptiveShrinkage {
        container = params.container_r
        cpus = 4
        memory = '6 GB'
    }

    withName: AddRegularizationToTestResults {
        container = params.container_r
        cpus = 2
        memory = '6 GB'
    }

    withName: CreateVolcanoPlots{
        container = params.container_r
        cpus = 2
        memory = '6 GB'
    }

    withName: ExtractGenomicFeatures{
        container = params.container_r
        cpus = 16
        memory = '12 GB'
    }

}

profiles {

    beyer_cluster {
        params{
           // Following values are hotfix for issue on our cluster, when running more than one FitModel processes per node
           // causes their CPU usage somehow collide. We therefore use artificially high memory requirement to avoid
           // more than one process to be submitted on one node.
           fit_model_cpus = 25
           fit_model_memory = '130 GB'

        }
        process {
          executor = 'slurm'
        }
    }
}

